// TOOD
// Minimize registers as ghosts.
// init_ctr can go ghost
// iv_reg   can go ghost
// key_ptr  could after a setup of expanded key
// Clense exactly where I should and no more.

/*

        AES CTR mode
        Brian Milnes
        18 May 2017
        
        i. Abstract

        This is the assembly code for AES running CTR, or counter mode.
 The counter is a bit underspecified in:

   NIST Special Publication 800-38A 
    Recommendation for Block 
    2001 Edition 
    Cipher Modes of Operation 
    Methods and Techniques 
    Dworkin

 This one is high bit 64 bits of initialization vector and a low
 64 bits of a counter.
 
	Table of Contents

 1.	Introduction
 2.	Declarations
 3.	CTREncryptOneBlockStdCall
 4.	CTR128Increment64StdCall
 6.	CTR128Increment128Reg
 7.	CTR128Increment128StdCall

*/

/*
	1.	Introduction

   This first Vale project of mine is carefully documented to make it simpler for someone
  else to get started with Vale.
 
  I have had to learn many things about vale to be able to go forward with this.

  a) Calling Conventions - see testctr.c but basically we're working on two conventions
     but only bothering to build x64 software at the moment. So you get STD Call conventions
     on Linux. See CTREncryptOneBlockStdCall for details.

  b) Test Code - there is no default test harness (c unit or such). See testctr.c
   in everest/vale/src/crypto/aes/. 

  c) Building - the test code is your default product to build. 
    cd ~/everest/vale; time scons obj/testctr.exe; obj/testctr.exe

  d) emacs - SCons is set up to build into the obj tree, so you don't have
     compilation error following. As most of your errors are from Danfy or
     Z3 anyway, it's not that useful.

  e) Instructions

    I have had to add four ways to insert instructions to make this work:

    incr64(inout operand dst:uint64)
    MovLow64To64(out operand dst:uint64, operand src:Quadword)
    MovLow64To128(inout operand dst: Quadword, operand src:uint64)
    Mov64ToHigh128(inout operand dst: Quadword, operand src:uint64)
    MOVHLPS(inout operand dst:Quadword, operand src:Quadword)
    MOVLHPS(inout operand dst:Quadword, operand src:Quadword)

    It is still unclear to me why add64 with a constant operand would not work.
    The moves are mostly MOVQ but it zeros the top of 128 bit registers.
    We want to be able to use 128 bit registers encrypted with the ctr (iv + int)
    to xor with plain or cypher text.

    Instructions go in  ~/everest/vale/src/arch/x64/decls64.vad.
    Their ADT is in     ~/everest/vale/src/arch/x64/def.s.dfy
      where they are checked for validity, have an observation taken, and are
      implemented on the abstract machine.
    They are then emitted to assemblers as text from printGcc.s.dfy and printMasm.s.dfy.

    Instruction selection wasted a good bit of time in that it's a bit tricky to
   get the right SSE (as they are called) 128 bit instructions working. Many
   zero the uper 64 bits (for pipeline efficiency) and the obvious MOVQ is not
   allowed to go XMM register to XMM register.

 f) In order to validate the assembler routines you are passing in and out
   ghost variables that you use in specifications.

 g) File structure - 
     crypto/aes/aes-x64/ctr.vad    - The assembly code.
     crypto/aes/ctr.s.dfy          - Functional specifications using dfy.
     crypto/aes/ctr_helpers.i.dfy  - Predicates that wrap up the functional specs
     crypto/aes/ctr_clean.s.dfy    - An attempt to make clean specifcations about
                                     data in use.

 h) Memory Model - TBD

 i) GCC assembly -
    The file ends up in obj/ctr-linux.S and remember the dst vs src argument order
     is sanely src -> dst, unlike the intel stuff.

 j) Good documents:
     https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf
     The architecture manual. Huge, hard to read. No examples.

     https://software.intel.com/sites/default/files/m/d/4/1/d/8/Introduction_to_x64_Assembly.pdf
      A nice place to start but not nearly enough information in it on how to read the
     architecture manual.

     file:///home/USER/everest/vale/doc/index.html - some minimal documentation on vale.

 k) Contradictions -

    It is quite easily possible to give Vale/Dafny a contradiction, particularly using
    the memory model. If so then you can "ensure false;" I test for this by putting in
    an "ensures false;".
 
*/


/*
        2. Declarations

*/

include "../../../arch/x64/decls.vad"
include "../../../arch/x64/decls64.vad"
include{:verbatim} "../../../arch/x64/print.s.dfy"
include{:verbatim} "../../../lib/util/dafny_wrappers.i.dfy"
include "aes.vad"
include{:verbatim} "ctr_helpers.i.dfy"
include{:verbatim} "../ctr.s.dfy" 
include{:verbatim} "proof.dfy"

#verbatim
module CTR {

import opened x64_def_s
import opened x64_vale_i
import opened x64_print_s
import opened dafny_wrappers_i
import opened x64_decls_i
import opened x64_decls64_i
import opened aes_vale
import opened CTRModule
import opened CTRHelpers
import opened proof

#endverbatim

/*
	3.	CTREncryptOneBlockStdCall

 Starting with https://en.wikipedia.org/wiki/X86_calling_conventions.
 Calling convention so we can call this from C for test.
  We should get up to four arguments on the stack in
 that order with x64 calling conventions.
  rdi == expanded_key pointer
  esi == counter pointer 
  rdx == input pointer
  rcx == output pointer
 Actually passing args in rdi, esi, rdx, rcx. 
 but gcc gives us esi but its zero extended in rsi.

 Encrypt one block, a test that I can correctly call AES from C.

*/

/*
procedure {:refined} {:timeLimitMultiplier 3} CTREncryptOneBlockStdCall(
   ghost key:seq(uint32), 
   ghost w:seq(uint32),
   ghost ctr_heap_id: heaplet_id,
   ghost key_heap_id: heaplet_id,
   ghost input:Quadword,
   ghost in_heap_id : heaplet_id,
   ghost out_heap_id: heaplet_id,
   ghost alg:Algorithm
   ) returns (
    ghost output:Quadword
  )
    reads
      rcx; rsi;

    modifies
        mem; efl; xmm0; xmm1; r8; xmm2; rdi; rdx; 

    requires 
     let key_ptr     := rdi;
     let ctr_ptr     := rsi; // moved into xmm0 as it's the input 
     let in_ptr      := rdx; // moved into xmm1 so it can be xord with xmm0.
     let out_ptr     := rcx; // has xmm0 stored into it.
     
     // AES requirements
     SeqLength(w) == 44;
     SeqLength(key) == 4;
     KeyExpansionPredicate(key, AES_128, w);
     ValidSrcAddrs(mem, key_heap_id, rdi, 128, Secret, 16*11);
     forall j :: 0 <= j <= 10 ==>
         mem[key_heap_id].quads[rdi + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);

     key_ptr % 16 == 0;
     ValidSrcAddr(mem, ctr_heap_id, ctr_ptr, 128, Public);
     ValidSrcAddr(mem, in_heap_id,   in_ptr, 128, Secret);
     ValidSrcAddr(mem, out_heap_id, out_ptr, 128, Public);

     out_heap_id != key_heap_id;
     out_heap_id != ctr_heap_id;
     out_heap_id != in_heap_id;

  ensures
     let out_ptr     := rcx; // has xmm0 stored into it.
     ValidSrcAddr(mem, out_heap_id, out_ptr, 128, Public);

     // The memory is written only on the output heap.
     mem == old(mem)[out_heap_id := mem[out_heap_id]];

    // The rest of the output heap is unchanged. 
    // Warning about mem64 can only be applied to Heaplet64 objects.
    // Bryan question.
    // The rest of the output heap is unchanged.
    //    forall a :: (a < out_ptr || a >= out_ptr + 128) && 
    //      old(mem)[out_heap_id].mem64?[a] ==>  mem[out_heap_id].mem64?[a] && 
    //      mem[out_heap_id].mem64[a] == old(mem)[out_heap_id].mem64[a];

    // No way this is right.
    // InputMatchesMemory(input,   mem[in_heap_id], ?);
    // InputMatchesMemory(counter, mem[ctr_heap_id], ?);
    //CTR_Encrypt_One_Block_Final(key,
    //                            input,
    //                            counter,
    //                          alg,
    //                            mem,
    //                           output_heap_id,
    //                           rcx,
    //                            output);
{
     Load128(xmm0, rsi, 0, Public, ctr_heap_id);                 // Load the counter into xmm0.
     Mov64(r8, rdi);                                             // key in r8, 16 byte aligned.
     AES128EncryptOneBlock(key, xmm0, w, Secret, key_heap_id);   // Encrypt the counter back into xmm0.
     Load128(xmm1, rdx,  0, Secret, in_heap_id);                 // Load the block of plaintext.
     Pxor(xmm0, xmm1);                                           // Xor the encrypted counter with the plaintext.
     Store128(rcx, xmm0, 0, Public, out_heap_id);                // Store the cyphertext.

  // Clean up state.
  Xor64(rdx,rdx);
  Xor64(rdi,rdi);
  Pxor(xmm0, xmm0);
  Pxor(xmm1, xmm1);
}
*/

// Decryption uses the same routine as we just xor again but the taints are different.

/*

        4. CTR128Increment64StdCall

   Do a 128 bit input/output but don't keep a 128 bit value in memory.
 In this case, read the int part of the counter, increment it, put it back in
 an output counter. Then copy over the IV part.

 Made for a C test case and learning.

*/

/*
procedure {:refined} CTR128Increment64StdCall(
   ghost  in_ctr_heap_id: heaplet_id,
   ghost  out_ctr_heap_id: heaplet_id
)
// Does this need a ghost output? Nah it's just test.

   modifies
        mem; efl; rdi; rsi; rdx;

   requires
     let in_ctr_ptr  := rdi;
     let out_ctr_ptr := rsi;
     ValidSrcAddr(mem,  in_ctr_heap_id, in_ctr_ptr, 64, Public);
     ValidSrcAddr(mem,  in_ctr_heap_id, in_ctr_ptr + 8, 64, Public);
     ValidDstAddr(mem, out_ctr_heap_id, out_ctr_ptr, 64);
     ValidDstAddr(mem, out_ctr_heap_id, out_ctr_ptr + 8, 64);
     in_ctr_ptr % 8 == 0;
     out_ctr_ptr % 8 == 0;
     in_ctr_heap_id != out_ctr_heap_id;

   ensures
     let in_ctr_ptr  := old(rdi);
     let out_ctr_ptr := old(rsi);

     // What properties do I want to prove?

     // I have not touched the stack as it is not in the modifies.
     // I have not changed the input. 
     // Implied by not changing the heaplet.
     //ValidSrcAddr(mem,  in_ctr_heap_id, in_ctr_ptr, 64, Public);
     //ValidSrcAddr(mem,  in_ctr_heap_id, in_ctr_ptr + 8, 64, Public);
     // I have valid output addresses, Src => Dst.
//     ValidSrcAddr(mem, out_ctr_heap_id, out_ctr_ptr, 64, Public);
//     ValidSrcAddr(mem, out_ctr_heap_id, out_ctr_ptr + 8, 64, Public);

    // 2 64 byte stretches are the only things changed.
//    Mem64ChangedOnlyIn(out_ctr_ptr, 2, out_ctr_heap_id, mem, old(mem));

     // High half of the output is the low half of the input.
//    mem[out_ctr_heap_id].mem64[out_ctr_ptr+8].t == old(mem)[in_ctr_heap_id].mem64[in_ctr_ptr+8].t;

    // High half of the output is the high half of the input + 1 mod 2^64.
//    mem[out_ctr_heap_id].mem64[out_ctr_ptr].v ==
//    BitwiseAdd64(old(mem)[in_ctr_heap_id].mem64[in_ctr_ptr].v, 1);

{
    ghost var in_ctr_ptr  := rdi;
    ghost var out_ctr_ptr := rsi;
 
    // Grab the counter low bits, add 1, store it.
    Load64 (rdx, rdi, 0, Public, in_ctr_heap_id);
    //Add64(rdx, 1); // Why will this not work?
    incr64 (rdx); 
    Store64(rsi, rdx, 0, Public, out_ctr_heap_id);

    // Grab the counter high bits, the IV, and store it.
    Load64 (rdx, rdi, 8, Public, in_ctr_heap_id);
    Store64(rsi, rdx, 8, Public, out_ctr_heap_id);

    // Clean up state.
    Xor64(rdi,rdi);
    Xor64(rsi,rsi);
    Xor64(rdx,rdx);
 }
*/


/*
	6. CTR128Increment128Reg

  This is the version that we really want: a no expense virtual function call
  to take an IV+Int counter in a 128 bit register and increment just the 64 bit
  int.

*/

/*
procedure {:refined} CTR128Increment128Reg(
   ghost  counter         : Quadword,
   inout  operand temp64  : uint64
)
returns (
  ghost incrcounter : Quadword
)
   requires
     CtrInReg(counter, xmm0);

   modifies
        efl; xmm0; xmm1;

   ensures
   IncrCtrInReg(counter, incrcounter, xmm0);
{
      MovLow64To64(temp64, xmm0);  // Grab counter half.
      MOVHLPS(xmm1, xmm0);         // Save IV half in temp128.
      incr64(temp64);
      MovLow64To128(xmm0, temp64); // But zeros top bytes.
      MOVLHPS(xmm0, xmm1);         // Put back the IV.

      Xor64(temp64, temp64);       // Clean up.
      Pxor(xmm1, xmm1); 
      incrcounter := xmm0;         // Specify what the results mean.
}
*/

/*
         7.	CTR128Increment128StdCall

  Call increment with correctly aligned 128 bit values in memory.
  Put them in a register, call the no overhead vale routine that is
  just going to be expaned inline.

  You end up with something like this:

CTR128Increment64StdCall:
  movq 0 (%rdi), %rdx
  add $1, %rdx
  movq %rdx, 0 (%rsi)
  movq 8 (%rdi), %rdx
  movq %rdx, 8 (%rsi)
  xor %rdi, %rdi
  xor %rsi, %rsi
  xor %rdx, %rdx
  ret 

*/

// The 128 bit standard call that sets up the 128 bit register of this.

/*
procedure {:refined} CTR128Increment128StdCall(
   ghost  counter : Quadword, 
   ghost  in_ctr_heap_id: heaplet_id,
   ghost  out_ctr_heap_id: heaplet_id
)
returns (
  ghost incrcounter : Quadword
)

   modifies
        mem; efl; rdi; rsi; rdx; xmm0; xmm1;

   requires
     let in_ctr_ptr  := rdi;
     let out_ctr_ptr := rsi;
   
   //  I have a counter in memory.
   CtrInMem(counter, rdi, in_ctr_heap_id, mem);
   // And a place two write out when incremented.
   ValidDstAddr(mem, out_ctr_heap_id, out_ctr_ptr, 128);

   in_ctr_heap_id != out_ctr_heap_id;

   ensures
     let in_ctr_ptr  := old(rdi);
     let out_ctr_ptr := old(rsi);
    // I am only changing 1 quad word.
     Mem128ChangedOnlyIn(out_ctr_ptr, 1, out_ctr_heap_id, mem, old(mem));
    // And I placed the incremented counter in it.
    CtrInMem(incrcounter, rsi, out_ctr_heap_id, mem);
{
      Load128(xmm0, rdi, 0, Public, in_ctr_heap_id);
      assert counter == xmm0;
      incrcounter := CTR128Increment128Reg(counter, rdx);
      Store128(rsi, xmm0, 0, Public, out_ctr_heap_id);
}

*/

// Dummy version.
//procedure {:refined} CTR128Increment128StdCall(
//   ghost  counter : Quadword, 
//   ghost  in_ctr_heap_id: heaplet_id,
//   ghost  out_ctr_heap_id: heaplet_id
//)
//returns (
//  ghost incrcounter : Quadword
//)
//{
//}

/*
         8.	CTR128Increment128StdCall

 A single routine version that does not give us a modular way to call
to increment a counter.

 The 128 bit standard call that sets up the 128 bit register of this.
*/

/*
procedure {:refined} CTR128Increment128StdCall(
   ghost  counter : Quadword, 
   ghost  in_ctr_heap_id: heaplet_id,
   ghost  out_ctr_heap_id: heaplet_id
)
returns (
  ghost incrcounter : Quadword
)

   modifies
        mem; efl; rdi; rsi; rdx; xmm0; xmm1;

   requires
     let in_ctr_ptr  := rdi;
     let out_ctr_ptr := rsi;
   
   //  I have a counter in memory.
   CtrInMem(counter, rdi, in_ctr_heap_id, mem);
   // And a place two write out when incremented.
   ValidDstAddr(mem, out_ctr_heap_id, out_ctr_ptr, 128);

   in_ctr_heap_id != out_ctr_heap_id;

   ensures
     let in_ctr_ptr  := old(rdi);
     let out_ctr_ptr := old(rsi);
     // I am only changing 1 quad word.
     Mem128ChangedOnlyIn(out_ctr_ptr, 1, out_ctr_heap_id, mem, old(mem));
     // And I placed the incremented counter in it.
     CtrInMem(incrcounter, rsi, out_ctr_heap_id, mem);
{
      Load128(xmm0, rdi, 0, Public, in_ctr_heap_id);
      assert counter == xmm0;
      MovLow64To64(rdx, xmm0);  // Grab counter half.
      MOVHLPS(xmm1, xmm0);      // Save IV half in temp128.
      incr64(rdx);
      MovLow64To128(xmm0, rdx); // But zeros top bytes.
      MOVLHPS(xmm0, xmm1);      // Put back the IV.
      Xor64(rdx, rdx);          // Clean up.
      Pxor(xmm1,xmm1);
      incrcounter := xmm0;
      Store128(rsi, xmm0, 0, Public, out_ctr_heap_id); // Put it back.
}
*/

// Old test stuff above this. 

procedure {:refined} CTR128Increment128(ghost g : G, ghost blocktobecopied : nat)

   reads    rdi; rsi; rdx; rcx; r8; r9; r12; mem;
   modifies efl; xmm4; xmm5; r13; r14;

   requires
      CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
      CtrInvBefore(rdx, xmm4, r12, r13, blocktobecopied);
 
   ensures
      CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
      CtrInvAfter(rdx, xmm4, r12, r13, blocktobecopied + 1);
{
      lemma_BitwiseAdd64();
      incr64(r13);              // Add 1 to the ctr.
      assert r13 == BitwiseAdd64(r12, blocktobecopied + 1);
      Mov64(r14,r13);           // Copy to bswap.
      BSwap64(r14);             
      MovLow64To128(xmm5, r14); // Put it in an xmm5 so we can move it to the high bits.
      MOVLHPS(xmm4, xmm5);      // Put back the ctr, iv untouched.

      Xor64(r14,r14);           // Clean up.
      Pxor(xmm5,xmm5);
}

// Can't use operands, instructions are refined.
procedure {:refined} {:bridge} {:timeLimitMultiplier 7}
    CTR128EncryptCopy(ghost g         : G,
                      ghost blocktobecopied : nat)

    reads    rdi; rsi; rdx; rcx; r8; r9; r12;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;


   requires
     CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
     CopyInvBefore(g,  rcx, rsi, r10, r9, r11, rdx, r12, xmm4, r13, rdi, mem, blocktobecopied);
     CtrInvBefore(rdx, xmm4, r12, r13, blocktobecopied);
     CTR_Encrypt_Upto_Done(g, rdx, r12, r9, mem, blocktobecopied);

   ensures
     CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
     CopyInvAfter(g, rcx, rsi, r10, r9, r11, rdx, r12, xmm4, r13, rdi, mem, old(mem), blocktobecopied + 1);
     CtrInvAfter(rdx, xmm4, r12, r13, blocktobecopied + 1);
     CTR_Encrypt_Upto_Done(g, rdx, r12, r9, mem, blocktobecopied + 1);
{
     Mov128(xmm0,xmm4); // Put the counter in xmm0.
     // input is in xmm0, so it output.
     // extended key ptr is in r8 
     ghost var xmm0_1 := Quadword(xmm0.lo,xmm0.mid_lo,xmm0.mid_hi,xmm0.hi);
     assert xmm0_1 == ctr_n(rdx, r12, blocktobecopied);

     AES128EncryptOneBlock(g.key, xmm0_1, g.exp_key, Secret, g.exp_key_heap);
     ghost var xmm0_2 := xmm0;
     assert xmm0_2 == AES_Encrypt(g.key, ctr_n(rdx, r12, blocktobecopied), g.alg);

     Load128(xmm1, r10, 0, Secret, g.inp_heap);
     ghost var xmm1_0 := xmm1;
     assert xmm1_0 == g.inp[blocktobecopied];

     Pxor(xmm1,xmm0);
     ghost var xmm1_1 := xmm1;
     assert xmm1_1 == QuadwordXor(g.inp[blocktobecopied], xmm0_2);
     lemma_CTR_Encrypt_length'(g.inp, g.key, g.alg, rdx, r12);
     // assume SeqLength(CTR_Encrypt(g.inp, g.key, g.alg, rdx, r12)) > blocktobecopied;
     lemma_CTR_Encrypt_Is_QuadwordXor_AES(g.inp, g.key, g.alg, rdx, r12);
     assert xmm1_1 == CTR_Encrypt(g.inp, g.key, g.alg, rdx, r12)[blocktobecopied];

     Store128(r11, xmm1, 0, Secret, g.out_heap);
     assert r11 == r9 + blocktobecopied * 16;
     assert mem[g.out_heap].quads[r9 + blocktobecopied * 16].v == xmm1_1;

     assert CTR_Encrypt(g.inp, g.key, g.alg, rdx, r12)[blocktobecopied] == 
                    mem[g.out_heap].quads[r9 + blocktobecopied * 16].v;
     lemma_CTR_Encrypt_Upto_Done(g, rdx, r12, r9, mem, blocktobecopied);

     CTR128Increment128(g, blocktobecopied);
     Pxor(xmm0,xmm0); // Clean up encrypted counter.
     Add64(r10, 16);
     Add64(r11, 16);
}

procedure {:timeLimitMultiplier 3} CTR128EncryptLoop(ghost g   : G,
                            operand key_ptr : uint64,
                            operand exp_key_ptr : uint64,
                            operand iv_reg : uint64,
                            operand inp_ptr : uint64,
                            operand inp_end_ptr : uint64,
                            operand out_ptr : uint64,
                            inout operand iv_ctr : Quadword,
                            operand init_ctr : uint64,
                            inout operand ctr : uint64,
                            inout operand curr_inp_ptr : uint64,
                            inout operand curr_out_ptr : uint64)
                         
   reads    rdi; rsi; rdx; rcx; r8; r9; r12;
   modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;

   requires
     BindRegsPhy1Curr(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr, @curr_inp_ptr, @curr_out_ptr);
     CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
     CopyInvBefore(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, 0);
     CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, 0);
     CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, 0);

   ensures
      BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
      CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
      CopyInvAfter(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, old(mem), SeqLength(g.inp));
      CtrInvAfter(iv_reg, iv_ctr, init_ctr, ctr, SeqLength(g.inp));
      CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
{
    ghost var blocktobecopied := 0;
    while (curr_inp_ptr < inp_end_ptr)
     invariant 
      BindRegsPhy1Curr(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr, @curr_inp_ptr, @curr_out_ptr);
      CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));

      (curr_inp_ptr < inp_end_ptr) ==>
       (CopyInvBefore(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, blocktobecopied) &&
       CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, blocktobecopied) &&
       CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, blocktobecopied));

      (curr_inp_ptr < inp_end_ptr) && (0 < blocktobecopied <= SeqLength(g.inp)) ==>
       (CopyInvAfter(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, old(mem), blocktobecopied) &&
       CtrInvAfter(iv_reg, iv_ctr, init_ctr, ctr, blocktobecopied) &&
       CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, blocktobecopied));

     (curr_inp_ptr >= inp_end_ptr) ==>
      (CopyInvAfter(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, old(mem), SeqLength(g.inp)) && 
       CtrInvAfter(iv_reg, iv_ctr, init_ctr, ctr, SeqLength(g.inp)) && 
       CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp)));

    decreases
      inp_end_ptr - curr_inp_ptr; // ignored but should be this
    {
     CTR128EncryptCopy(g, blocktobecopied);
     blocktobecopied := blocktobecopied + 1;
    }
}

// Everything is in a register, the CTR is built in xmm4.
procedure CTR128EncryptStart(ghost g   : G,
                            operand key_ptr : uint64,
                            operand exp_key_ptr : uint64,
                            operand iv_reg : uint64,
                            operand inp_ptr : uint64,
                            operand inp_end_ptr : uint64,
                            operand out_ptr : uint64,
                            inout operand iv_ctr : Quadword,
                            operand init_ctr : uint64,
                            inout operand ctr : uint64)
    reads    rdi; rsi; rdx; rcx; r8; r9; r12;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;

    requires
     BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
     CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
     CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, 0);
     CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, 0);

    ensures
     BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
     CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
     InputInOutputMem(g, out_ptr, iv_reg, init_ctr, ctr, iv_ctr, key_ptr, mem, SeqLength(g.inp));
     CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
{
    // Set up a curr and out ptrs.
    Mov64(r10, rcx);
    Mov64(r11, r9);
    assert CopyInvBefore(g, inp_ptr, inp_end_ptr, r10, out_ptr, r11, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, 0);
    CTR128EncryptLoop(g, key_ptr, exp_key_ptr, iv_reg, inp_ptr, inp_end_ptr, out_ptr, iv_ctr, init_ctr, ctr, r10, r11);
}

procedure {:refined} {:bridge} CTR128SetupCTR(ghost g : G)
  reads    rdi; rsi; rdx; rcx; r8; r9; r12; mem;
  modifies efl; xmm4; xmm5; r13; r14;

  requires
    CTRInvInit(g, rdi, r8, rcx, rsi, r9, rdx, r13, mem, old(mem));

  ensures
    CTRInvInit(g, rdi, r8, rcx, rsi, r9, rdx, r13, mem, old(mem));
    CtrInvBefore(rdx, xmm4, r12, r13, 0);
{
    reveal lower64;
    reveal upper64;
    reveal BitwiseAdd64;
    assert BitwiseAdd64(r12, 0) == r12;
   // Build the counter.
    MovLow64To128(xmm4, rdx); // Put the iv in low xmm4.
    Mov64(r13,r12);           // Copy the initial big endian counter to ctr(r13).
    Mov64(r14,r13);           // Copy ctr to temp reg.
    BSwap64(r14);             // Make it little endian just for xor.
    MovLow64To128(xmm5, r14); // Put the ctr in low xmm5.
    MOVLHPS(xmm4, xmm5);      // Put back into the top of xmm4.

    Pxor(xmm5,xmm5);          // Clean up.
    Xor64(r14,r14);           // Clean up temp.
}

procedure CTR128EncryptReg(ghost g : G,
                           operand key_ptr : uint64,
                           operand exp_key_ptr : uint64,
                           operand iv_reg : uint64,
                           operand inp_ptr : uint64,
                           operand inp_end_ptr : uint64,
                           operand out_ptr : uint64,
                           inout operand iv_ctr : Quadword,
                           operand init_ctr : uint64,
                           inout operand ctr : uint64)
    reads    rdi; rsi; rdx; rcx; r8; r9; r12;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;

    requires
      BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
      CTRInvInit(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, ctr, mem, old(mem));

    ensures
      BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
      CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
      InputInOutputMem(g, out_ptr, iv_reg, init_ctr, ctr, iv_ctr, key_ptr, mem, SeqLength(g.inp));
      CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
{
   CTR128SetupCTR(g);
   assert CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, 0);
   assert CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
   CTR128EncryptStart(g, key_ptr, exp_key_ptr, iv_reg, inp_ptr, inp_end_ptr, out_ptr, iv_ctr, init_ctr, ctr);
}

// Linux only calling sequence so far, just in case I need to pass more arguments and hit the stack.
// Plus windows may shuffle registers and so on.
procedure CTR128EncryptStdcall(ghost g : G)
    reads    rdi; rsi; rdx; rcx; r8; r9;  stack;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r12; r13; r14;

   requires/ensures
    let key_ptr := rdi; let exp_key_ptr := r8; let iv_reg := rdx; 
    let inp_ptr := rcx; let inp_end_ptr := rsi; let out_ptr := r9; let iv_ctr := xmm4; 
    let ctr := r14;
    CTRInvInit(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, ctr, mem, old(mem));
    HasStackSlot(stack,0);
    HasStackSlot(stack,1);

   ensures
    let key_ptr := rdi; let exp_key_ptr := r8;  let iv_reg := rdx; 
    let inp_ptr := rcx; let inp_end_ptr := rsi; let out_ptr := r9; let iv_ctr := xmm4;
    let init_ctr := r12; let ctr := r14;
    CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
    InputInOutputMem(g, out_ptr, iv_reg, init_ctr, ctr, iv_ctr, key_ptr, mem, SeqLength(g.inp));
    CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
{
  LoadStack64(r12, 0);  // Get the initial counter from the stack.
  CTR128EncryptReg(g, rdi, r8, rdx, rcx, rsi, r9, xmm4, r12, r13);
}

// Dummy so validation always runs when I'm not calling all of the procedures.
//procedure {:refined} CTR128EncryptStdcall() requires false;  ensures true; {}

#verbatim
} // end module CTR
#endverbatim
