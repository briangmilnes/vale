/*

        AES CTR mode
        Brian Milnes
        18 May 2017
        
        i. Abstract

        This is the assembly code for AES running CTR, or counter mode.
 The counter is a bit underspecified in:

   NIST Special Publication 800-38A 
    Recommendation for Block 
    2001 Edition 
    Cipher Modes of Operation 
    Methods and Techniques 
    Dworkin

 This one is high bit 64 bits of initialization vector and a low
 64 bits of a counter. But does have test vectors which we call
 in testctr.c.
 

	Table of Contents

 1.	Introduction
 2.	Declarations
 3.	CTREncryptOneBlockStdCall
 4.	CTR128Increment64StdCall
 6.	CTR128Increment128Reg
 7.	CTR128EncryptStart
 8.	CTR128SetupCTR
 9.	CTR128CleanReg
 10.	CTR128EncryptReg
 11.	CTR128EncryptStdcall

*/

/*
	1.	Introduction

   This first Vale project of mine is carefully documented to make it simpler for someone
  else to get started with Vale.
 
  I have had to learn many things about vale to be able to go forward with this.

  a) Calling Conventions - see testctr.c but basically we're working on two conventions
     but only bothering to build x64 software at the moment. So you get STD Call conventions
     on Linux. 

  b) Test Code - there is no default test harness (c unit or such). See testctr.c
   in everest/vale/src/crypto/aes/. 

  c) Building - the test code is your default product to build. 
    cd ~/everest/vale; time scons obj/testctr.exe; obj/testctr.exe

  d) emacs - tags is available and compilation error following almost works with
      .emacs code from me. 

  e) Instructions

    I have had to add four ways to insert instructions to make this work:

    incr64(inout operand dst:uint64)
    MovLow64To64(out operand dst:uint64, operand src:Quadword)
    MovLow64To128(inout operand dst: Quadword, operand src:uint64)
    Mov64ToHigh128(inout operand dst: Quadword, operand src:uint64)
    MOVHLPS(inout operand dst:Quadword, operand src:Quadword)
    MOVLHPS(inout operand dst:Quadword, operand src:Quadword)

    The moves are mostly MOVQ but it zeros the top of 128 bit registers.
    We want to be able to use 128 bit registers encrypted with the ctr (iv + int)
    to xor with plain or cypher text.

    Instructions go in  ~/everest/vale/src/arch/x64/decls64.vad.
    Their ADT is in     ~/everest/vale/src/arch/x64/def.s.dfy
      where they are checked for validity, have an observation taken, and are
      implemented on the abstract machine.
    They are then emitted to assemblers as text from printGcc.s.dfy and printMasm.s.dfy.

    Instruction selection wasted a good bit of time in that it's a bit tricky to
   get the right SSE (as they are called) 128 bit instructions working. Many
   zero the uper 64 bits (for pipeline efficiency) and the obvious MOVQ is not
   allowed to go XMM register to XMM register.

 f) File structure - 
     crypto/aes/aes-x64/ctr.vad    - The assembly code.
     crypto/aes/ctr.s.dfy          - Functional specifications using dfy.
     crypto/aes/ctr_helpers.i.dfy  - Predicates that wrap up the functional specs

 g) :refined and :bridge are extremely dangerous and necessary while vale runs on Dafny.
  They speed up instruction proof but TOTALLY BREAK operand binding, forcing you to go
  back to register naming. 
 
 h) I often work on dafny theorems in a separate file included only here called proof.dfny.
    This improves separate compilation time.

 i) GCC assembly -
    The file ends up in obj/ctr-linux.S and remember the dst vs src argument order
     is sanely src -> dst, unlike the intel stuff.
    One can generate the C code's assembly to look at the calling conventions but
      must turn off O2.

 j) Good documents:
     https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf
     The architecture manual. Huge, hard to read. No examples.

     https://software.intel.com/sites/default/files/m/d/4/1/d/8/Introduction_to_x64_Assembly.pdf
      A nice place to start but not nearly enough information in it on how to read the
     architecture manual.

     file:///home/USER/everest/vale/doc/index.html - some minimal documentation on vale.

 k) Contradictions -

    It is quite easily possible to give Vale/Dafny a contradiction, particularly using
    the memory model. If so then you can "ensure false;" I test for this by putting in
    an "ensures false;".
 
 l) We also have 'assume' so one can assume a theorem statement and then prove based on this.

 m) Vale/Dafny.../Z3 is not necessarily proving lemmas in order in that the addition of following
   lemmas may increase what Z3 gets and cause it to time out on a previous lemma.x

 n) Binding is in general really weak in Vale's design. It's not just that :refined breaks things.
   The binding from require, requires/ensures and ensures are ALL different. And they don't
   have scope over the body either. In practice you need to get them bound to a register or
   constrain the complete graph of inequalities of your registers or Vale can't prove that
   writing RLedZeppelin is not going to modify RRollingStones.

   To work around these problems, I define two binding predicates and constrain the operator names.
  Ugly but effective.

   Another problem is that the reads/modifies clauses are literal register names. Very ugly.

 o) This file will take about five minutes of Z3 time on a pretty standard workstation ala
   2017. This is because Vale/Dafny/Boogie/Z3 has produced bloated files of Z3 clauses. In
   this case up to 200 MB of memory is used. Clause size and control are dramatically reduced
   in Vale/F* and so refined can go away for example.

 p) I built a ghost g: G datatype to carry my specifications without lots of arguments. However,
   one can not really put the output vector in this. I was hoping that I could simply add
   specifications to g.output and that would be sufficient. However, Vale requires that I'm
   not modifying the structure of this with assignments like g.outp := (g.out[6] := 9).

   So one really has to use return (ghost output : Seq<>) and even write lemmas that
   do the assignment.

*/


/*
        2. Declarations

*/

include "../../../arch/x64/decls.vad"
include "../../../arch/x64/decls64.vad"
include{:verbatim} "../../../arch/x64/print.s.dfy"
include{:verbatim} "../../../lib/util/dafny_wrappers.i.dfy"
include "aes.vad"
include{:verbatim} "ctr_helpers.i.dfy"
include{:verbatim} "../ctr.s.dfy" 
include{:verbatim} "proof.dfy" 

#verbatim
module CTR {

import opened x64_def_s
import opened x64_vale_i
import opened x64_print_s
import opened dafny_wrappers_i
import opened x64_decls_i
import opened x64_decls64_i
import opened aes_vale
import opened CTRModule
import opened CTRHelpers
import opened proof

#endverbatim

/*
	3.	CTREncryptOneBlockStdCall

 Starting with https://en.wikipedia.org/wiki/X86_calling_conventions.
 Calling convention so we can call this from C for test.
  We should get up to four arguments on the stack in
 that order with x64 calling conventions.
  rdi == expanded_key pointer
  esi == counter pointer 
  rdx == input pointer
  rcx == output pointer
 Actually passing args in rdi, esi, rdx, rcx. 
 but gcc gives us esi but its zero extended in rsi.

 Encrypt one block, a test that I can correctly call AES from C.

*/

procedure {:refined} {:timeLimitMultiplier 3} CTREncryptOneBlockStdCall(
   ghost key:seq(uint32), 
   ghost w:seq(uint32),
   ghost ctr_heap_id: heaplet_id,
   ghost key_heap_id: heaplet_id,
   ghost input:Quadword,
   ghost in_heap_id : heaplet_id,
   ghost out_heap_id: heaplet_id,
   ghost alg:Algorithm
   ) returns (
    ghost output:Quadword
  )
    reads
      rcx; rsi;

    modifies
        mem; efl; xmm0; xmm1; r8; xmm2; rdi; rdx; 

    requires 
     let key_ptr     := rdi;
     let ctr_ptr     := rsi; // moved into xmm0 as it's the input 
     let in_ptr      := rdx; // moved into xmm1 so it can be xord with xmm0.
     let out_ptr     := rcx; // has xmm0 stored into it.
     
     // AES requirements
     SeqLength(w) == 44;
     SeqLength(key) == 4;
     KeyExpansionPredicate(key, AES_128, w);
     ValidSrcAddrs(mem, key_heap_id, rdi, 128, Secret, 16*11);
     forall j :: 0 <= j <= 10 ==>
         mem[key_heap_id].quads[rdi + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);

     key_ptr % 16 == 0;
     ValidSrcAddr(mem, ctr_heap_id, ctr_ptr, 128, Public);
     ValidSrcAddr(mem, in_heap_id,   in_ptr, 128, Secret);
     ValidSrcAddr(mem, out_heap_id, out_ptr, 128, Public);

     out_heap_id != key_heap_id;
     out_heap_id != ctr_heap_id;
     out_heap_id != in_heap_id;

  ensures
     let out_ptr     := rcx; // has xmm0 stored into it.
     ValidSrcAddr(mem, out_heap_id, out_ptr, 128, Public);

     // The memory is written only on the output heap.
     mem == old(mem)[out_heap_id := mem[out_heap_id]];
{
     Load128(xmm0, rsi, 0, Public, ctr_heap_id);                 // Load the counter into xmm0.
     Mov64(r8, rdi);                                             // key in r8, 16 byte aligned.
     AES128EncryptOneBlock(key, xmm0, w, Secret, key_heap_id);   // Encrypt the counter back into xmm0.
     Load128(xmm1, rdx,  0, Secret, in_heap_id);                 // Load the block of plaintext.
     Pxor(xmm0, xmm1);                                           // Xor the encrypted counter with the plaintext.
     Store128(rcx, xmm0, 0, Public, out_heap_id);                // Store the cyphertext.
}

// Decryption uses the same routine as we just xor again but the taints are different.

/*

	4.	CTR128Increment128

*/

procedure {:refined} CTR128Increment128(ghost g : G, ghost blocktobecopied : nat)

   reads    rdi; rsi; rdx; rcx; r8; r9; r12; mem;
   modifies efl; xmm4; xmm5; r13; r14;

   requires
      CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
      CtrInvBefore(rdx, xmm4, r12, r13, blocktobecopied);
 
   ensures
      CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
      CtrInvAfter(rdx, xmm4, r12, r13, blocktobecopied + 1);
{
      lemma_BitwiseAdd64();
      incr64(r13);              // Add 1 to the ctr.
      assert r13 == BitwiseAdd64(r12, blocktobecopied + 1);
      Mov64(r14,r13);           // Copy to bswap.
      BSwap64(r14);             
      MovLow64To128(xmm5, r14); // Put it in an xmm5 so we can move it to the high bits.
      MOVLHPS(xmm4, xmm5);      // Put back the ctr, iv untouched.
}

/*

	5.	CTR128EncryptCopy

*/

// Can't use operands, instructions are refined.

procedure {:refined} {:bridge} {:timeLimitMultiplier 7}
    CTR128EncryptCopy(ghost g         : G,
                      ghost blocktobecopied : nat)

    reads    rdi; rsi; rdx; rcx; r8; r9; r12;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;


   requires
     CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
     CopyInvBefore(g,  rcx, rsi, r10, r9, r11, rdx, r12, xmm4, r13, rdi, mem, blocktobecopied);
     CtrInvBefore(rdx, xmm4, r12, r13, blocktobecopied);
     CTR_Encrypt_Upto_Done(g, rdx, r12, r9, mem, blocktobecopied);

   ensures
     CTRInv(g, rdi, r8, rcx, rsi, r9, rdx, xmm4, r13, mem, old(mem));
     CopyInvAfter(g, rcx, rsi, r10, r9, r11, rdx, r12, xmm4, r13, rdi, mem, old(mem), blocktobecopied + 1);
     CtrInvAfter(rdx, xmm4, r12, r13, blocktobecopied + 1);
     CTR_Encrypt_Upto_Done(g, rdx, r12, r9, mem, blocktobecopied + 1);
{
     Mov128(xmm0,xmm4); // Put the counter in xmm0.
     // input is in xmm0, so it output.
     // extended key ptr is in r8 
     ghost var xmm0_1 := Quadword(xmm0.lo,xmm0.mid_lo,xmm0.mid_hi,xmm0.hi);
     assert xmm0_1 == ctr_n(rdx, r12, blocktobecopied);

     AES128EncryptOneBlock(g.key, xmm0_1, g.exp_key, Secret, g.exp_key_heap);
     ghost var xmm0_2 := xmm0;
     assert xmm0_2 == AES_Encrypt(g.key, ctr_n(rdx, r12, blocktobecopied), g.alg);

     Load128(xmm1, r10, 0, Secret, g.inp_heap);
     ghost var xmm1_0 := xmm1;
     assert xmm1_0 == g.inp[blocktobecopied];

     Pxor(xmm1,xmm0);
     ghost var xmm1_1 := xmm1;
     assert xmm1_1 == QuadwordXor(g.inp[blocktobecopied], xmm0_2);
     lemma_CTR_Encrypt_length'(g.inp, g.key, g.alg, rdx, r12);
     // assume SeqLength(CTR_Encrypt(g.inp, g.key, g.alg, rdx, r12)) > blocktobecopied;
     lemma_CTR_Encrypt_Is_QuadwordXor_AES(g.inp, g.key, g.alg, rdx, r12);
     assert xmm1_1 == CTR_Encrypt(g.inp, g.key, g.alg, rdx, r12)[blocktobecopied];

     Store128(r11, xmm1, 0, Secret, g.out_heap);
     assert r11 == r9 + blocktobecopied * 16;
     assert mem[g.out_heap].quads[r9 + blocktobecopied * 16].v == xmm1_1;

     assert CTR_Encrypt(g.inp, g.key, g.alg, rdx, r12)[blocktobecopied] == 
                    mem[g.out_heap].quads[r9 + blocktobecopied * 16].v;
     lemma_CTR_Encrypt_Upto_Done(g, rdx, r12, r9, mem, blocktobecopied);

     CTR128Increment128(g, blocktobecopied);
     Add64(r10, 16);
     Add64(r11, 16);
}

/*
	
	6.	CTR128EncryptLoop

*/

procedure {:timeLimitMultiplier 3} CTR128EncryptLoop(ghost g   : G,
                            operand key_ptr : uint64,
                            operand exp_key_ptr : uint64,
                            operand iv_reg : uint64,
                            operand inp_ptr : uint64,
                            operand inp_end_ptr : uint64,
                            operand out_ptr : uint64,
                            inout operand iv_ctr : Quadword,
                            operand init_ctr : uint64,
                            inout operand ctr : uint64,
                            inout operand curr_inp_ptr : uint64,
                            inout operand curr_out_ptr : uint64)
                         
   reads    rdi; rsi; rdx; rcx; r8; r9; r12;
   modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;

   requires
     BindRegsPhy1Curr(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr, @curr_inp_ptr, @curr_out_ptr);
     CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
     CopyInvBefore(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, 0);
     CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, 0);
     CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, 0);

   ensures
      BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
      CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
      CopyInvAfter(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, old(mem), SeqLength(g.inp));
      CtrInvAfter(iv_reg, iv_ctr, init_ctr, ctr, SeqLength(g.inp));
      CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
{
    ghost var blocktobecopied := 0;
    while (curr_inp_ptr < inp_end_ptr)
     invariant 
      BindRegsPhy1Curr(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr, @curr_inp_ptr, @curr_out_ptr);
      CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));

      (curr_inp_ptr < inp_end_ptr) ==>
       (CopyInvBefore(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, blocktobecopied) &&
       CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, blocktobecopied) &&
       CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, blocktobecopied));

      (curr_inp_ptr < inp_end_ptr) && (0 < blocktobecopied <= SeqLength(g.inp)) ==>
       (CopyInvAfter(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, old(mem), blocktobecopied) &&
       CtrInvAfter(iv_reg, iv_ctr, init_ctr, ctr, blocktobecopied) &&
       CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, blocktobecopied));

     (curr_inp_ptr >= inp_end_ptr) ==>
      (CopyInvAfter(g, inp_ptr, inp_end_ptr, curr_inp_ptr, out_ptr, curr_out_ptr, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, old(mem), SeqLength(g.inp)) && 
       CtrInvAfter(iv_reg, iv_ctr, init_ctr, ctr, SeqLength(g.inp)) && 
       CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp)));

    decreases
      inp_end_ptr - curr_inp_ptr; // ignored but should be this
    {
     CTR128EncryptCopy(g, blocktobecopied);
     blocktobecopied := blocktobecopied + 1;
    }
}

/*

	7.	CTR128EncryptStart

 Everything is in a register, the CTR is built in xmm4.
*/

procedure CTR128EncryptStart(ghost g   : G,
                            operand key_ptr : uint64,
                            operand exp_key_ptr : uint64,
                            operand iv_reg : uint64,
                            operand inp_ptr : uint64,
                            operand inp_end_ptr : uint64,
                            operand out_ptr : uint64,
                            inout operand iv_ctr : Quadword,
                            operand init_ctr : uint64,
                            inout operand ctr : uint64) 
                     returns (ghost output : seq(Quadword))
    reads    rdi; rsi; rdx; rcx; r8; r9; r12;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r13; r14;

    requires
     BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
     CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
     CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, 0);
     CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, 0);

    ensures
     BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
     CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
     InputInOutputMem(g, out_ptr, iv_reg, init_ctr, ctr, iv_ctr, key_ptr, mem, SeqLength(g.inp));
     CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
     output == CTR_Encrypt(g.inp, g.key, g.alg, iv_reg, init_ctr);
{
    // Set up a curr and out ptrs.
    Mov64(r10, rcx);
    Mov64(r11, r9);
    assert CopyInvBefore(g, inp_ptr, inp_end_ptr, r10, out_ptr, r11, iv_reg, init_ctr, iv_ctr, ctr, key_ptr, mem, 0);
    CTR128EncryptLoop(g, key_ptr, exp_key_ptr, iv_reg, inp_ptr, inp_end_ptr, out_ptr, iv_ctr, init_ctr, ctr, r10, r11);
    output := lemma_CTROutput(g, iv_reg, init_ctr, out_ptr, mem);
}

/*

		8.	CTR128SetupCTR

*/

procedure {:refined } {:bridge} CTR128SetupCTR(ghost g : G)
  reads    rdi; rsi; rdx; rcx; r8; r9; r12; mem;
  modifies efl; xmm4; xmm5; r13; r14;

  requires
    CTRInvInit(g, rdi, r8, rcx, rsi, r9, rdx, r13, mem, old(mem));

  ensures
    CTRInvInit(g, rdi, r8, rcx, rsi, r9, rdx, r13, mem, old(mem));
    CtrInvBefore(rdx, xmm4, r12, r13, 0);
{
    reveal lower64;
    reveal upper64;
    reveal BitwiseAdd64;
    assert BitwiseAdd64(r12, 0) == r12;
   // Build the counter.
    MovLow64To128(xmm4, rdx); // Put the iv in low xmm4.
    Mov64(r13,r12);           // Copy the initial big endian counter to ctr(r13).
    Mov64(r14,r13);           // Copy ctr to temp reg.
    BSwap64(r14);             // Make it little endian just for xor.
    MovLow64To128(xmm5, r14); // Put the ctr in low xmm5.
    MOVLHPS(xmm4, xmm5);      // Put back into the top of xmm4.
}

/*

		9.	CTR128CleanReg

   Clean out any state that might break taint analysis.
*/

procedure {:refined} {:bridge}  CTR128CleanReg()
  modifies efl; xmm0; xmm4; xmm5; rdi; rdx; r12; r14;
{
  // Clean up once at the end.
  Pxor(xmm0,xmm0); // encrypted counter
  Pxor(xmm4,xmm4); // iv_ctr
  Pxor(xmm5,xmm5); // tmp
  Xor64(rdi, rdi); // key
  Xor64(rdx, rdx); // ctr
  Xor64(r12,r12);  // init_ctr.
  Xor64(r14,r14);  // ctr
}


/*

		10.	 CTR128EncryptReg

   If you call from assembly, call here. 

*/

procedure CTR128EncryptReg(ghost g : G,
                           operand key_ptr : uint64,
                           operand exp_key_ptr : uint64,
                           operand iv_reg : uint64,
                           operand inp_ptr : uint64,
                           operand inp_end_ptr : uint64,
                           operand out_ptr : uint64,
                           inout operand iv_ctr : Quadword,
                           operand init_ctr : uint64,
                           inout operand ctr : uint64)
                  returns (ghost output : seq(Quadword))

    reads    rsi; rdx; rcx; r8; r9; 
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r12; r13; r14; rdi; rdx;

    requires
      BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
      CTRInvInit(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, ctr, mem, old(mem));

    ensures
      BindRegsPhy1(@key_ptr, @exp_key_ptr, @iv_reg, @inp_ptr, @inp_end_ptr, @out_ptr, @iv_ctr, @init_ctr, @ctr);
      CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
      InputInOutputMem(g, out_ptr, iv_reg, init_ctr, ctr, iv_ctr, key_ptr, mem, SeqLength(g.inp));
      CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
      output == CTR_Encrypt(g.inp, g.key, g.alg, iv_reg, init_ctr);
{
   CTR128SetupCTR(g);
   assert CtrInvBefore(iv_reg, iv_ctr, init_ctr, ctr, 0);
   assert CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
   output := CTR128EncryptStart(g, key_ptr, exp_key_ptr, iv_reg, inp_ptr, inp_end_ptr, out_ptr, iv_ctr, init_ctr, ctr);
}

/*
		11.	CTR128EncryptStdcall

   If you call from C, call here.  See testctr.c for the calling conventions linked.

   Linux only calling sequence so far.
*/

procedure CTR128EncryptStdcall(ghost g : G) returns (ghost output : seq(Quadword))
    reads    rsi; rcx; r8; r9;  stack;
    modifies mem; efl; xmm0; xmm1; xmm2; xmm4; xmm5; r10; r11; r12; r13; r14; rdi; rdx;

   requires
    let key_ptr := rdi; let exp_key_ptr := r8; let iv_reg := rdx; 
    let inp_ptr := rcx; let inp_end_ptr := rsi; let out_ptr := r9; let iv_ctr := xmm4; 
    let ctr := r14;
    CTRInvInit(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, ctr, mem, old(mem));
    HasStackSlot(stack,0);
    HasStackSlot(stack,1);

{
  LoadStack64(r12, 0);  // Get the initial counter from the stack.
  output :=  CTR128EncryptReg(g, rdi, r8, rdx, rcx, rsi, r9, xmm4, r12, r13);

  ghost var key_ptr := rdi; ghost var exp_key_ptr := r8;  ghost var iv_reg := rdx; 
  ghost var inp_ptr := rcx; ghost var inp_end_ptr := rsi; ghost var out_ptr := r9; ghost var iv_ctr := xmm4;
  ghost var init_ctr := r12; ghost var ctr := r14;
  // Have to assert these down here as we are cleaning out secret state.
  assert CTRInv(g, key_ptr, exp_key_ptr, inp_ptr, inp_end_ptr, out_ptr, iv_reg, iv_ctr, ctr, mem, old(mem));
  assert InputInOutputMem(g, out_ptr, iv_reg, init_ctr, ctr, iv_ctr, key_ptr, mem, SeqLength(g.inp));
  assert CTR_Encrypt_Upto_Done(g, iv_reg, init_ctr, out_ptr, mem, SeqLength(g.inp));
  assert output == CTR_Encrypt(g.inp, g.key, g.alg, iv_reg, init_ctr);
  CTR128CleanReg();
}

// Dummy so validation always runs when I'm not calling all of the procedures.
//procedure {:refined} CTR128EncryptStdcall() requires false;  ensures true; {}

#verbatim
} // end module CTR
#endverbatim
