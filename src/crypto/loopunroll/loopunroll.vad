/*
  Do some testing on building and proving unrolled loops.

*/

include "../../arch/x64/decls.vad"
include "../../arch/x64/decls64.vad"
include{:verbatim} "../../arch/x64/print.s.dfy"
include{:verbatim} "../../lib/util/dafny_wrappers.i.dfy"
include{:verbatim} "loopunroll.s.dfy" 
include{:verbatim} "seqcomp.s.dfy" 
include{:verbatim} "addrlogic.s.dfy" 
include "loopunroll.proven.vad"
include "loopunroll.unrolled.vad"

#verbatim
module LoopUnroll {

import opened x64_def_s
import opened x64_vale_i
import opened x64_print_s
import opened dafny_wrappers_i
import opened x64_decls_i
import opened x64_decls64_i
import opened LoopUnrollModule
import opened seqcomp
import opened addrlogic
import opened LoopUnrollProven
import opened LoopUnrollUnrolled
#endverbatim

// Atributes:
// input pointer
// end pointer
// 8 byte aligned
// 64 byte chunks guaranteed
// Proof
//  Still using guard
//  Can't prove how many iterations either.
//  Can't prove input pointer == end pointer.
procedure IterateBy64EndUpOverShouldBeOn(ghost id : heaplet_id)
  modifies efl; r8;
  reads    rsi;
  requires r8 <= rsi;
  requires r8 % 8 == 0;
  requires (rsi - r8) % 64 == 0;
  requires rsi + 64 < 0x1_0000_0000_0000_0000;
  ensures r8 >= rsi;
{
  lemma_BitwiseAdd64(); // for the decreases clause.

  ghost var io_ptr := r8;
  ghost var io_end := rsi;                   // Comment.
  ghost var items  := (rsi - r8) / 8; // How many items do we do, of size 8.
  ghost var blocks := (rsi - r8) / 64; // How many blocks to do.
  ghost var count := 0;                      // How many have we done.
  
  while (r8 < rsi)
   invariant r8 <= rsi + 64;
   invariant r8 == io_ptr + count * 64;
   invariant (r8 - io_ptr) % 64 == 0;
   invariant (rsi - r8) % 64 == 0;
   invariant count == (r8 - io_ptr) / 64;
   invariant count + (rsi - r8) / 64 == blocks;
   decreases rsi - r8;
  {
    AddN(r8,64);
    count := count + 1;
  } 
//  assert count == (rsi - io_ptr) / 64;
  assert count == blocks;
//  assert r8 == rsi;
} 

/*
procedure IterateBy64EndUpOn(ghost id : heaplet_id)
  modifies efl; r8;
  reads    rsi;
  requires r8 < rsi;
  requires (rsi - r8) % 64 == 0;
  requires rsi < 0x1_0000_0000_0000_0000;
  ensures r8 == rsi;
{
  lemma_BitwiseAdd64(); // for the decreases clause.
  while (r8 < rsi)
   invariant r8 <= rsi;
   decreases rsi - r8;
  {
    AddN(r8,64);
  } 
} 
*/

/*
procedure IncrementUint64Unrolled8(ghost id : heaplet_id)
    modifies mem; efl; rdx; rcx; r8; r9; r10; r11; r12; r13; r14; r15; rbx;
    requires/ensures ValidSrcAlAddrs64(mem, id, addrs64(old(r8), 8), Public);

    ensures  UpdatesAlAddrs64(old(mem), mem, id, addrs64(old(r8), 8), Public,
                  Plus8(old(mem), id, addrs64(old(r8),8), 8));
    ensures OnlyAddrs64Modified(old(mem), mem, id, addrs64(old(r8), 8));
    ensures r8 == BitwiseAdd64(old(r8),64);
{
  IncrementUint64Unrolled(id, 7);
  AddN(r8, 64);
}
*/

/*
// Try and validate a loop with input in 64 byte chunks.
procedure IncrementVectorUnrolled8(ghost id : heaplet_id) 
    reads    rdi; rsi;
    modifies stack; mem; efl; rdx; rcx; r8; r9; r10; r11; r12; r13; r14; r15; rbx; rsi; rdi;
//    requires/ensures HasStackSlots(stack,16);
    requires rsi > 0;
    requires rdi > 0;
    requires rdi + rsi + 64 < 0x1_0000_0000_0000_0000; // Take a guard.
    requires ValidSrcAlAddrs64(mem, id, addrs64(rdi, rsi), Public);
    requires rsi % 64 == 0; // So we can chew off 8 * 8 segements.

{
  lemma_BitwiseAdd64(); // for the decreases clause.
  lemma_ValidSrcAlAddrs64(mem, id, addrs64(rdi, rsi), Public);

  // rdi - points to an aligned vector of 
  // rsi - size of the vector in bytes.
  //  SaveRegisters();
  ghost var io_ptr := rdi;
  ghost var io_cnt_bytes := rsi;
  ghost var max_count := io_cnt_bytes / 64;
  Mov64(r8, rdi);
  Add64(rsi, rdi); // RSI is now the end pointer.
  ghost var count := 0;
  assert r8 == io_ptr + count * 64;
  assert r8 + 64 < 0x1_0000_0000_0000_0000;
  while (r8 < rsi)
   invariant 
     r8  == io_ptr + count * 64;
     io_ptr <= r8;
//     r8 <= io_ptr + max_count * 64;
//     count <= max_count;
   decreases rsi - r8;
   {
     AddN(r8, 64);
   //IncrementUint64Unrolled8(id);
     count := count + 1;
   }
   assert count == max_count;
//  assert io_ptr <= r8 <= io_ptr + io_cnt_bytes + 64;
//  RestoreRegisters();
}
*/

/*
procedure IncrementVectorUnrolled4(ghost id : heaplet_id) 
    reads    rdi; rsi;
    modifies stack; mem; efl; rdx; rcx; r8; r9; r10; r11; r12; r13; r14; r15; rbx; rsi; rdi;
    requires HasStackSlots(stack,16);
    requires r8 < 0x1_0000_0000_0000_0000 - rsi * 8;
    requires ValidSrcAlAddrs64(mem, id, addrs64(rdi, rsi), Public);
{
  SaveRegisters();
  Mov64(r8, rdi);
  Add64(rsi, rdi);
  while (r8 < rsi)
   invariant true;   
   decreases r8 - rsi;
   {
     assume r8 < 0x1_0000_0000_0000_0000 - 4 * 8;
     IncrementUint64Unrolled(id, 4, 4);
   }
  assume HasStackSlots(stack,16);
  RestoreRegisters();
}

procedure IncrementVectorUnrolled8(ghost id : heaplet_id) 
    reads    rdi; rsi;
    modifies stack; mem; efl; rdx; rcx; r8; r9; r10; r11; r12; r13; r14; r15; rbx; rsi; rdi;
    requires HasStackSlots(stack,16);
    requires r8 < 0x1_0000_0000_0000_0000 - rsi * 8;
    requires ValidSrcAlAddrs64(mem, id, addrs64(rdi, rsi), Public);
{
  SaveRegisters();
  Mov64(r8, rdi);
  Add64(rsi, rdi);
  while (r8 < rsi)
   invariant true;   
   decreases r8 - rsi;
   {
     assume r8 < 0x1_0000_0000_0000_0000 - 8* 8;
     IncrementUint64Unrolled(id, 8, 8);
   }
  assume HasStackSlots(stack,16);
  RestoreRegisters();
}
*/

#verbatim
} 
#endverbatim
